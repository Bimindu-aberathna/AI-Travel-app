{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bimindu-aberathna/AI-Travel-app/blob/main/Fake_Review_Detection_with_CNN_%26_Bi_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcQmCj7Yitga"
      },
      "outputs": [],
      "source": [
        "# 1. Imports packages\n",
        "import kagglehub\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import sentencepiece as spm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import (\n",
        "    Embedding, Conv1D, MaxPooling1D,\n",
        "    LSTM, Dense, Dropout, GlobalMaxPooling1D,\n",
        "    Bidirectional\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import (\n",
        "    Embedding, Conv1D, MaxPooling1D,\n",
        "    LSTM, Dense, Dropout, GlobalMaxPooling1D,\n",
        "    Bidirectional\n",
        ")"
      ],
      "metadata": {
        "id": "Jy5Gx9pPm0nY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 Download & Load\n",
        "path = kagglehub.dataset_download(\"mexwell/fake-reviews-dataset\")\n",
        "file_path = os.path.join(path, \"fake reviews dataset.csv\")\n",
        "df = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "ZRc9uL-kiwcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 Quick Inspect\n",
        "print(\"Original label counts:\\n\", df['label'].value_counts(), \"\\n\")\n",
        "\n",
        "#Limit if you wanna fine-tune. Other-wise take stupid amount of time. But less data=less accuracy\n",
        "#df = df.head(2000)\n",
        "# print(\"Using top-1000 label counts:\\n\", df['label'].value_counts(), \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_o_FNfrQiwU0",
        "outputId": "8f4fc38e-9323-4b33-cd0e-a1c9328112e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original label counts:\n",
            " label\n",
            "CG    20216\n",
            "OR    20216\n",
            "Name: count, dtype: int64 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Map text labels to numeric/bool\n",
        "label_map = {'CG': 1, 'OR': 0}\n",
        "df['label_num'] = df['label'].map(label_map)\n",
        "print(\"Mapped label counts:\\n\", df['label_num'].value_counts(), \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_pfX_fQiwLn",
        "outputId": "c32d6187-b604-43d0-dc2a-77427d30b6c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mapped label counts:\n",
            " label_num\n",
            "1    20216\n",
            "0    20216\n",
            "Name: count, dtype: int64 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Clean Text\n",
        "def clean_text(s):\n",
        "    s = s.lower()\n",
        "    s = re.sub(r\"<.*?>\", \"\", s)\n",
        "    s = re.sub(r\"http\\S+|www\\S+\", \"\", s)\n",
        "    s = re.sub(r\"[^a-z\\s]\", \"\", s)\n",
        "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
        "\n",
        "df['text'] = df['text_'].apply(clean_text)"
      ],
      "metadata": {
        "id": "97pBKVX7iwEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Train/Test Split (stratified on numeric labels)\n",
        "train_df, test_df = train_test_split(\n",
        "    df, test_size=0.2,\n",
        "    stratify=df['label_num'],\n",
        "    random_state=42\n",
        ")\n",
        "y_train = train_df['label_num'].values\n",
        "y_test  = test_df ['label_num'].values"
      ],
      "metadata": {
        "id": "sP4ixCK7jKpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Train SentencePiece on train only. THis is for tokenization. Su-word tokenization\n",
        "train_df['text'].to_csv('train_corpus.txt', index=False, header=False)\n",
        "spm.SentencePieceTrainer.Train(\n",
        "    input='train_corpus.txt',\n",
        "    model_prefix='spm_bpe',\n",
        "    vocab_size=10000,\n",
        "    character_coverage=1.0,\n",
        "    model_type='bpe',\n",
        "    control_symbols=['<pad>']\n",
        ")\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load('spm_bpe.model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h343UFpCjKhc",
        "outputId": "9f3d7d42-8d21-4e85-fb90-a184d3447a76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RY8FsSpEoaLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight"
      ],
      "metadata": {
        "id": "UMgouad5kbxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Encode & Pad\n",
        "maxlen = 150\n",
        "def encode_and_pad(texts):\n",
        "    seqs = [sp.encode(t, out_type=int) for t in texts]\n",
        "    return pad_sequences(seqs, padding='post', maxlen=maxlen)\n",
        "\n",
        "X_train = encode_and_pad(train_df['text'])\n",
        "X_test  = encode_and_pad(test_df ['text'])"
      ],
      "metadata": {
        "id": "t5LViR0vjKY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import LeakyReLU,BatchNormalization"
      ],
      "metadata": {
        "id": "C3cIVwGYne6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Build CNN + LSTM Model\n",
        "vocab_size = sp.get_piece_size()\n",
        "embedding_dim = 128\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, mask_zero=True),\n",
        "    Conv1D(128, 3, activation='relu', padding='same'),\n",
        "    Bidirectional(LSTM(64, return_sequences=True)),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(64),\n",
        "    LeakyReLU(alpha=0.1),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "#model.summary()"
      ],
      "metadata": {
        "id": "R_giobajjZfu",
        "outputId": "121247ec-75c5-416a-b922-fbd45423e5e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Train\n",
        "# history = model.fit(\n",
        "#     X_train, y_train,\n",
        "#     epochs=5,\n",
        "#     batch_size=32,\n",
        "#     validation_data=(X_test, y_test)\n",
        "# )\n",
        "# compute the weights for each class 0 and 1\n",
        "cw = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "\n",
        "#try to get a better value by changing Epoches and size folks\n",
        "model.fit(X_train, y_train,\n",
        "          epochs=13, batch_size=32,\n",
        "          validation_data=(X_test, y_test),\n",
        "          class_weight={i: w for i, w in enumerate(cw)})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUg2VsjdjZMG",
        "outputId": "180c41f3-8fbe-43f2-b810-5f930fcd7ef8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:938: UserWarning: Layer 'conv1d_31' (of type Conv1D) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1011/1011\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 19ms/step - accuracy: 0.8325 - loss: 0.3421 - val_accuracy: 0.9413 - val_loss: 0.1659\n",
            "Epoch 2/13\n",
            "\u001b[1m1011/1011\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.9612 - loss: 0.0992 - val_accuracy: 0.9450 - val_loss: 0.1434\n",
            "Epoch 3/13\n",
            "\u001b[1m1011/1011\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - accuracy: 0.9832 - loss: 0.0494 - val_accuracy: 0.9487 - val_loss: 0.1761\n",
            "Epoch 4/13\n",
            "\u001b[1m1011/1011\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - accuracy: 0.9913 - loss: 0.0261 - val_accuracy: 0.9461 - val_loss: 0.1846\n",
            "Epoch 5/13\n",
            "\u001b[1m1011/1011\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 18ms/step - accuracy: 0.9942 - loss: 0.0172 - val_accuracy: 0.9439 - val_loss: 0.2177\n",
            "Epoch 6/13\n",
            "\u001b[1m1011/1011\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - accuracy: 0.9963 - loss: 0.0107 - val_accuracy: 0.9470 - val_loss: 0.2442\n",
            "Epoch 7/13\n",
            "\u001b[1m1011/1011\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 18ms/step - accuracy: 0.9967 - loss: 0.0087 - val_accuracy: 0.9440 - val_loss: 0.2868\n",
            "Epoch 8/13\n",
            "\u001b[1m1011/1011\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - accuracy: 0.9974 - loss: 0.0075 - val_accuracy: 0.9515 - val_loss: 0.2991\n",
            "Epoch 9/13\n",
            "\u001b[1m1011/1011\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 18ms/step - accuracy: 0.9974 - loss: 0.0074 - val_accuracy: 0.9456 - val_loss: 0.2926\n",
            "Epoch 10/13\n",
            "\u001b[1m1011/1011\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 18ms/step - accuracy: 0.9978 - loss: 0.0065 - val_accuracy: 0.9477 - val_loss: 0.2755\n",
            "Epoch 11/13\n",
            "\u001b[1m1011/1011\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - accuracy: 0.9989 - loss: 0.0033 - val_accuracy: 0.9440 - val_loss: 0.2842\n",
            "Epoch 12/13\n",
            "\u001b[1m1011/1011\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - accuracy: 0.9985 - loss: 0.0040 - val_accuracy: 0.9447 - val_loss: 0.3139\n",
            "Epoch 13/13\n",
            "\u001b[1m1011/1011\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 19ms/step - accuracy: 0.9993 - loss: 0.0024 - val_accuracy: 0.9467 - val_loss: 0.3591\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7dbbe0bd7a50>"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. Evaluate & Confusion Matrix\n",
        "y_pred_prob = model.predict(X_test)\n",
        "y_pred      = (y_pred_prob > 0.5).astype(int).flatten()\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, digits=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hG5Py8QjY_O",
        "outputId": "fd7d4d01-63dc-49be-8d87-1b6f7789df36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
            "\n",
            "Confusion Matrix:\n",
            "[[3921  123]\n",
            " [ 348 3695]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9185    0.9696    0.9433      4044\n",
            "           1     0.9678    0.9139    0.9401      4043\n",
            "\n",
            "    accuracy                         0.9418      8087\n",
            "   macro avg     0.9431    0.9418    0.9417      8087\n",
            "weighted avg     0.9431    0.9418    0.9417      8087\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2OhAAwaOjY2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sS4z9--JjYsP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}